{"id":"req_llm-1br","title":"[Feature] #233: Refresh the model list manually","description":"https://github.com/agentjido/req_llm/issues/233 - Feature request allowing users to refresh the model list manually","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:29.413672-06:00","updated_at":"2025-11-29T10:06:29.413672-06:00","labels":["feature"]}
{"id":"req_llm-2bl","title":"Add pre-push git hook for mix format check","description":"Add a pre-push git hook that runs 'mix format --check-formatted' before pushing code. This prevents CI failures due to formatting differences between Elixir versions. The hook should be installed automatically or documented in CONTRIBUTING.md.","status":"open","priority":3,"issue_type":"task","created_at":"2025-11-29T12:26:07.201822-06:00","updated_at":"2025-11-29T12:26:13.79525-06:00"}
{"id":"req_llm-2ha","title":"[Feature] #213: List models available given current keys","description":"https://github.com/agentjido/req_llm/issues/213 - Feature request to list available models based on the current API keys","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:50.117168-06:00","updated_at":"2025-11-29T10:06:50.117168-06:00","labels":["feature"]}
{"id":"req_llm-3fe","title":"[Feature] #236: Provide a mock provider like vercel ai","description":"https://github.com/agentjido/req_llm/issues/236 - Feature request to provide a mock provider similar to Vercel AI","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:24.232138-06:00","updated_at":"2025-11-29T10:06:24.232138-06:00","labels":["feature"]}
{"id":"req_llm-4ek","title":"[Feature] #230: Support multiple system messages in Context","description":"https://github.com/agentjido/req_llm/issues/230 - Feature request to enable support of multiple system messages within the context","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:34.583645-06:00","updated_at":"2025-11-29T10:06:34.583645-06:00","labels":["feature"]}
{"id":"req_llm-67i","title":"[Bug] #240: Mix task to sync models aren't loaded","description":"https://github.com/agentjido/req_llm/issues/240 - Mix task for syncing models is not loading correctly","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:13.937274-06:00","updated_at":"2025-11-29T10:06:13.937274-06:00","labels":["bug"]}
{"id":"req_llm-6li","title":"PR #246: Fix Responses API tool encoding to use flat structure","description":"https://github.com/agentjido/req_llm/pull/246 - Fixes tool encoding for OpenAI Responses API to use correct flat structure format instead of nested format","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-11-29T10:04:45.403322-06:00","updated_at":"2025-11-29T11:57:13.534593-06:00"}
{"id":"req_llm-8ku","title":"[Feature] #229: Support additional provider options for openrouter","description":"https://github.com/agentjido/req_llm/issues/229 - Feature request to add support for more provider options for openrouter, usage and plugins","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:39.764456-06:00","updated_at":"2025-11-29T10:06:39.764456-06:00","labels":["feature"]}
{"id":"req_llm-afq","title":"[Bug] #199: Venice AI provider showing as metadata-only","description":"https://github.com/agentjido/req_llm/issues/199 - Venice AI provider incorrectly displayed as metadata-only","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:07:00.469235-06:00","updated_at":"2025-11-29T10:07:00.469235-06:00","labels":["bug"]}
{"id":"req_llm-eya","title":"[Bug] #239: Environment variables are forcefully loaded","description":"https://github.com/agentjido/req_llm/issues/239 - Environment variables being loaded in a forceful manner","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:19.075083-06:00","updated_at":"2025-11-29T10:06:19.075083-06:00","labels":["bug"]}
{"id":"req_llm-fom","title":"[Bug] #243: Multi-tool calls answer formatting","description":"https://github.com/agentjido/req_llm/issues/243 - Bug related to formatting of answers from multi-tool calls","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:03.644509-06:00","updated_at":"2025-11-29T10:06:03.644509-06:00","labels":["bug"]}
{"id":"req_llm-ioq","title":"[Feature] #203: Add Bumblebee Provider","description":"https://github.com/agentjido/req_llm/issues/203 - Feature request to add support for the Bumblebee provider","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:55.289726-06:00","updated_at":"2025-11-29T10:06:55.289726-06:00","labels":["feature"]}
{"id":"req_llm-js5","title":"PR #245: Add Azure OpenAI provider","description":"https://github.com/agentjido/req_llm/pull/245 - Adds Azure AI provider supporting OpenAI and Anthropic models hosted on Azure with deployment-based routing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-29T10:04:50.587174-06:00","updated_at":"2025-11-29T12:17:37.179049-06:00","closed_at":"2025-11-29T12:17:37.179049-06:00"}
{"id":"req_llm-ogc","title":"[Feature] #226: Add url_context support in Google provider","description":"https://github.com/agentjido/req_llm/issues/226 - Feature request to include support for url_context within the Google provider's context","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:44.940665-06:00","updated_at":"2025-11-29T10:06:44.940665-06:00","labels":["feature"]}
{"id":"req_llm-p2y","title":"[Bug] #242: Req.TransportError with GPT-5.1 and structured object","description":"https://github.com/agentjido/req_llm/issues/242 - Transport error :closed constantly happening with GPT-5.1 and structured objects","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-29T10:06:08.792625-06:00","updated_at":"2025-11-29T10:06:08.792625-06:00","labels":["bug"]}
